{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from numpy.random import RandomState\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import random, math, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size', 40, \"\"\"Number of images to process in a batch.\"\"\")\n",
    "TOWER_NAME = 'tower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read olivetti face dataset\n",
    "def read_facedata(shuffle=True, batch_size=20):\n",
    "    numpy_rng = RandomState(123)\n",
    "    dataset = fetch_olivetti_faces(shuffle=True, random_state=numpy_rng)\n",
    "\n",
    "    # faces = dataset.data\n",
    "    images = dataset.images\n",
    "    label = dataset.target\n",
    "\n",
    "    np.random.shuffle(images)\n",
    "\n",
    "    digit_indices = [np.where(label == i)[0] for i in range(40)]\n",
    "\n",
    "    train_data =[]\n",
    "    train_label =[]\n",
    "    # valid_data = []\n",
    "    # valid_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    for d in range(len(digit_indices)):\n",
    "        # train_data += d[0:6]\n",
    "        for i in range(10):\n",
    "            if i <8:\n",
    "                index = digit_indices[d][i]\n",
    "                train_data.append(images[index])\n",
    "                train_label.append(label[index])\n",
    "            # elif i >= 6 and i <8:\n",
    "            # valid_data.append(images[digit_indices[d][i]])\n",
    "            # valid_label.append(label[digit_indices[d][i]])\n",
    "            else :\n",
    "                test_data.append(images[digit_indices[d][i]])\n",
    "                test_label.append(label[digit_indices[d][i]])\n",
    "\n",
    "    return train_data, train_label, test_data, test_label\n",
    "\n",
    "#create pairs for siamese network input\n",
    "def create_pairs(x, digit_indices):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(40)]) - 1\n",
    "    for d in range(40):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i+1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, 40)\n",
    "            dn = (d + inc) % 40\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "#get next batch in pair\n",
    "def next_batch(start,end,inputs,labels):\n",
    "    input1 = inputs[start:end,0]\n",
    "    input2 = inputs[start:end,1]\n",
    "    y= np.reshape(labels[start:end],(len(range(start,end)),1))\n",
    "    return input1,input2,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# referred from Tensorflow tutorial https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\n",
    "def _activation_summary(x):\n",
    "    tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "    \n",
    "def _variable(name, shape, initializer):\n",
    "    dtype = tf.float32\n",
    "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "    return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    dtype = tf.float32\n",
    "    var = tf.get_variable(name=name,\n",
    "      shape=shape,\n",
    "      initializer=tf.random_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "\n",
    "def inference(images):\n",
    "    images = tf.reshape(images, shape=[-1, 64, 64, 1])\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "#         kernel = _variable_with_weight_decay('weights',\n",
    "#                                          shape=[5, 5, 1, 64],\n",
    "#                                          stddev=.01,\n",
    "#                                          wd=0.0)\n",
    "#         conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "#         biases = tf.get_variable(name='biases', shape=[64], initializer=tf.constant_initializer(0.1))\n",
    "#         pre_activation = tf.nn.bias_add(conv, biases)\n",
    "#         conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        conv1 = conv_relu(images, [5, 5, 1, 64], [64])\n",
    "        _activation_summary(conv1)\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "#         kernel = _variable_with_weight_decay('weights',\n",
    "#                                              shape=[3, 3, 64, 32],\n",
    "#                                              stddev=1e-2,\n",
    "#                                              wd=0.0)\n",
    "#         conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "#         biases = tf.get_variable(name='biases', shape=[32], initializer=tf.constant_initializer(0.1))\n",
    "#         pre_activation = tf.nn.bias_add(conv, biases)\n",
    "#         conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        conv2 = conv_relu(pool1, [3, 3, 64, 32], [32])\n",
    "        _activation_summary(conv2)\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "    with tf.variable_scope('local2') as scope:\n",
    "        reshape = tf.reshape(pool2, [FLAGS.batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                          stddev=0.01, wd=0.004)\n",
    "        biases = tf.get_variable(name='biases', shape=[384], initializer=tf.constant_initializer(0.1))\n",
    "        local2 = tf.add(tf.matmul(reshape, weights), biases, name=scope.name)\n",
    "        _activation_summary(local2)\n",
    "    return local2\n",
    "\n",
    "# getting probability as defined by the paper\n",
    "def sigmoid_prob(feature1, feature2):\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', [384, 1], stddev=1/384.0, wd=0.0)\n",
    "        feature = tf.abs(feature1 - feature2)\n",
    "        predictionProb = tf.sigmoid(tf.matmul(feature, weights), name=None)\n",
    "        # print(predictionProb.get_shape())\n",
    "        # _activation_summary(softmax_linear)\n",
    "    return predictionProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#referenced from site https://github.com/jeromeyoon/Tensorflow-siamese/blob/master/main.py\n",
    "#train and test data\n",
    "X_train, y_train, X_test, y_test = read_facedata(batch_size=FLAGS.batch_size)\n",
    "\n",
    "# create training + test positive and negative pairs\n",
    "digit_indices = []\n",
    "for j in range(40):\n",
    "    digit_indices.append([i for i, x in enumerate(y_train) if x == j])\n",
    "tr_pairs, tr_y = create_pairs(X_train, digit_indices)\n",
    "\n",
    "digit_indices = []\n",
    "for j in range(40):\n",
    "    digit_indices.append([i for i, x in enumerate(y_test) if x == j])\n",
    "te_pairs, te_y = create_pairs(X_test, digit_indices)\n",
    "\n",
    "#placeholders for twin network input\n",
    "images_L = tf.placeholder(tf.float32,shape=([FLAGS.batch_size, 64,64]),name='L')\n",
    "images_R = tf.placeholder(tf.float32,shape=([FLAGS.batch_size, 64,64]),name='R')\n",
    "labels = tf.placeholder(tf.float32,shape=([FLAGS.batch_size, 1]),name='gt')\n",
    "\n",
    "with tf.variable_scope(\"siamese\") as scope:\n",
    "    model1= inference(images_L)\n",
    "    scope.reuse_variables()\n",
    "    model2 = inference(images_R)\n",
    "\n",
    "#similiarity score\n",
    "predictionProb  = sigmoid_prob(model1,model2)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cross_entropy_mean = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictionProb, labels=labels, name='cross_entropy_per_example'), name='cross_entropy') \n",
    "tf.add_to_collection('losses', cross_entropy_mean)\n",
    "loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "starter_learning_rate = 0.1\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "decay_steps = int(100)\n",
    "lr = tf.train.exponential_decay(starter_learning_rate,global_step, decay_steps, 0.1,staircase=True)\n",
    "tf.summary.scalar('learning_rate', lr)\n",
    "momentum = 0.5\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=momentum, use_nesterov=False).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  time: 11.333193 loss 1.24735 acc 47.524\n",
      "epoch 1  time: 11.449757 loss 1.21673 acc 49.640\n",
      "epoch 2  time: 10.031758 loss 1.18591 acc 50.000\n",
      "epoch 3  time: 9.821339 loss 1.15587 acc 50.000\n",
      "Accuract training set 56.250\n",
      "Accuract test set 50.000\n"
     ]
    }
   ],
   "source": [
    "#Evaluation metric accuracy\n",
    "def compute_accuracy(prediction,labels):\n",
    "    return labels[prediction.ravel() < 0.5].mean()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(100):\n",
    "        avg_loss = 0.\n",
    "        avg_accuracy = 0.\n",
    "        total_batch = int(len(X_train)/FLAGS.batch_size)\n",
    "        start_time = time.time()\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            s  = i * FLAGS.batch_size\n",
    "            e = (i+1) * FLAGS.batch_size\n",
    "            # Fit training using batch data\n",
    "            input1, input2, y = next_batch(s, e, tr_pairs, tr_y)\n",
    "            # print(y.get_shape())\n",
    "            _, loss_value, predict=sess.run([optimizer,loss,predictionProb], feed_dict={images_L:input1,images_R:input2 ,labels:y})\n",
    "            feature1=model1.eval(feed_dict={images_L:input1})\n",
    "            feature2=model2.eval(feed_dict={images_R:input2})\n",
    "            tr_acc = compute_accuracy(predict, y)\n",
    "#             print(tr_acc)\n",
    "            avg_loss += loss_value\n",
    "            avg_accuracy +=tr_acc*100\n",
    "        #print('epoch %d loss %0.2f' %(epoch,avg_loss/total_batch))\n",
    "        duration = time.time() - start_time\n",
    "        print('epoch %d  time: %f loss %0.5f acc %0.3f' %(epoch,duration,avg_loss/(total_batch), avg_accuracy/total_batch))\n",
    "    total_batch = int(len(X_train)/FLAGS.batch_size)\n",
    "    te_acc = 0.\n",
    "    for i in range(total_batch):\n",
    "        s  = i * FLAGS.batch_size\n",
    "        e = (i+1) * FLAGS.batch_size\n",
    "        input1, input2, y = next_batch(s, e, tr_pairs, tr_y)\n",
    "        predict=predictionProb.eval(feed_dict={images_L:input1,images_R:input2,labels:y})\n",
    "        tr_acc += compute_accuracy(predict,y)\n",
    "    print('Accuract training set %0.3f' % (100 * tr_acc/total_batch))\n",
    "\n",
    "    # Test model\n",
    "    total_batch = int(len(X_test)/FLAGS.batch_size)\n",
    "    te_acc = 0.\n",
    "    for i in range(total_batch):\n",
    "        s  = i * FLAGS.batch_size\n",
    "        e = (i+1) * FLAGS.batch_size\n",
    "        # Fit training using batch data\n",
    "        input1, input2, y = next_batch(s, e, te_pairs, te_y)\n",
    "        predict=predictionProb.eval(feed_dict={images_L:input1,images_R:input2,labels:y})\n",
    "        te_acc += compute_accuracy(predict,y)\n",
    "    print('Accuract test set %0.3f' % (100 * te_acc/total_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
